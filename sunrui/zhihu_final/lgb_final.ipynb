{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autotime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test b shape =  (1141718, 7)\n",
      "time: 1min 34s\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "#显示所有列\n",
    "pd.set_option('display.max_columns', None)\n",
    "#显示所有行\n",
    "pd.set_option('display.max_rows', None)\n",
    "#设置value的显示长度为100，默认为50\n",
    "pd.set_option('max_colwidth',100)\n",
    "\n",
    "def reduce_mem_usage(df, verbose=True):\n",
    "    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "    start_mem = df.memory_usage().sum() / 1024 ** 2\n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtypes\n",
    "        if col_type in numerics:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)\n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)\n",
    "    end_mem = df.memory_usage().sum() / 1024 ** 2\n",
    "    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (\n",
    "            start_mem - end_mem) / start_mem))\n",
    "    return df\n",
    "\n",
    "invite_info = pd.read_hdf('./my_feat/convert_train.h5', key='data')\n",
    "invite_info_evaluate = pd.read_hdf('./my_feat/convert_test_b.h5', key='data')\n",
    "\n",
    "print('test b shape = ', invite_info_evaluate.shape)\n",
    "\n",
    "member_feat = pd.read_hdf('./feats/member_feat.h5', key='data')\n",
    "\n",
    "invite_info = invite_info.merge(member_feat, 'left', 'author_id')\n",
    "invite_info_evaluate = invite_info_evaluate.merge(member_feat, 'left', 'author_id')\n",
    "\n",
    "invite_info_evaluate['label'] = -1\n",
    "\n",
    "all_invite_data = pd.concat([invite_info,invite_info_evaluate],ignore_index=True)\n",
    "all_invite_data['anthor_count'] = all_invite_data.groupby('author_id')['author_id'].transform('count')\n",
    "all_invite_data['question_count'] = all_invite_data.groupby('question_id')['author_id'].transform('count')\n",
    "\n",
    "invite_info = all_invite_data[all_invite_data['label'] != -1].reset_index(drop=True)\n",
    "invite_info_evaluate = all_invite_data[all_invite_data['label'] == -1].drop('label',axis=1).reset_index(drop=True)\n",
    "\n",
    "question_feat = pd.read_hdf('./feats/question_feat.h5', key='data')\n",
    "\n",
    "member_question_feat = pd.read_hdf('./feats/member_question_feat_final.h5', key='data')\n",
    "\n",
    "invite_info['author_question_id'] = invite_info['author_id'] + invite_info['question_id']\n",
    "invite_info_evaluate['author_question_id'] = invite_info_evaluate['author_id'] + invite_info_evaluate['question_id']\n",
    "\n",
    "train = invite_info.merge(question_feat, 'left', 'question_id')\n",
    "train = train.merge(member_question_feat, 'left', 'author_question_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 3min 55s\n"
     ]
    }
   ],
   "source": [
    "train_user_current_answer_count =  pd.read_hdf('./my_feat/user_current_answer_count.h5', key='data')\n",
    "\n",
    "train_user_current_answer_count.head()\n",
    "train_user_current_answer_count.columns = ['question_id', 'author_id', 'current_answer_count']\n",
    "\n",
    "train = train.merge(train_user_current_answer_count, how='left', on=['question_id', 'author_id'])\n",
    "\n",
    "current_invite_count = pd.read_hdf('./my_feat/current_invite_count_b.h5',key='data') # 这里是所有的数据 可以直接和test进行merge\n",
    "\n",
    "train = train.merge(current_invite_count, how='left', on=['question_id', 'author_id'])\n",
    "\n",
    "current_invite_q_count = pd.read_hdf('./my_feat/current_invite_q_count_test_b.h5',key='data') # 这里是所有的数据 可以直接和test进行merge\n",
    "\n",
    "current_invite_q_count.columns=['question_id','author_id','current_invite_q_count']\n",
    "\n",
    "train = train.merge(current_invite_q_count, how='left', on=['question_id', 'author_id'])\n",
    "\n",
    "topic_cv_probs_train_df = pd.read_hdf('./my_feat/topic_cv_probs_train_df.h5',key='data')\n",
    "train = pd.concat([train, topic_cv_probs_train_df],axis=1)\n",
    "\n",
    "# 读取历史topic交集特征\n",
    "history_topic_with_current_topic_train = pd.read_hdf('./my_feat/history_topic_with_current_topic_train.h5', key='data')\n",
    "history_topic_with_current_topic_train = history_topic_with_current_topic_train[['question_id','author_id','intersect1d_topic_nums']]\n",
    "\n",
    "history_topic_vec_distance = pd.read_hdf('./my_feat/history_topic_vec_distance.h5',key='data')\n",
    "\n",
    "train = train.merge(history_topic_with_current_topic_train, how='left', on=['question_id', 'author_id'])\n",
    "train = train.merge(history_topic_vec_distance, how='left', on=['question_id', 'author_id'])\n",
    "\n",
    "train_time_gap = pd.read_hdf('./my_feat/train_time_gap.h5', key='data')\n",
    "train = pd.concat([train, train_time_gap], axis=1)\n",
    "\n",
    "history_topic_vec_cv_train = pd.read_hdf('./my_feat/history_topic_vec_cv_train_1118.h5', key='data')\n",
    "history_topic_vec_cv_train.columns = ['old_idx', 'history_topic_vec_probs']\n",
    "train = pd.concat([train, history_topic_vec_cv_train], axis=1)\n",
    "\n",
    "history_invite_situation_train = pd.read_hdf('./my_feat/current_invite_user_situation_train.h5', key='data')\n",
    "train = train.merge(history_invite_situation_train, how='left', on=['question_id', 'author_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mem. usage decreased to 24.81 Mb (66.7% reduction)\n",
      "Mem. usage decreased to 32.49 Mb (66.7% reduction)\n",
      "time_diff_train shape =  (9489162, 2)\n",
      "time: 3min 43s\n"
     ]
    }
   ],
   "source": [
    "count_and_unique_feature_train_1124 = pd.read_hdf('./my_feat/count_and_unique_feature_train_final_train.h5', key='data')\n",
    "\n",
    "train = pd.concat([train, count_and_unique_feature_train_1124], axis=1)\n",
    "\n",
    "uid_seq_feature_train = pd.read_hdf('./my_feat/uid_seq_feature_train_b.h5', key='data')\n",
    "\n",
    "train = pd.concat([train, uid_seq_feature_train], axis=1)\n",
    "\n",
    "topic_convert_df = pd.read_hdf('./my_feat/topic_convert_train_df.h5', key='data')\n",
    "train = pd.concat([train, topic_convert_df], axis=1)\n",
    "\n",
    "topic_count_statistics_train = pd.read_hdf('./my_feat/topic_count_statistics_train.h5', key='data')\n",
    "train = pd.concat([train, topic_count_statistics_train], axis=1)\n",
    "\n",
    "q_title_kw_convert = pd.read_hdf('./my_feat/q_title_kw_convert.h5', key='data')\n",
    "train = pd.concat([train, q_title_kw_convert], axis=1)\n",
    "\n",
    "kw_count_statistics_train = pd.read_hdf('./my_feat/kw_count_statistics_train.h5', key='data')\n",
    "train = pd.concat([train, kw_count_statistics_train], axis=1)\n",
    "\n",
    "deepwalk_question = pd.read_pickle('./deepwalk/author_id_question_id_question_id_deepwalk_8_final.pkl')\n",
    "deepwalk_question = reduce_mem_usage(deepwalk_question)\n",
    "train = train.merge(deepwalk_question, 'left', 'question_id')\n",
    "\n",
    "deepwalk_author = pd.read_pickle('./deepwalk/author_id_question_id_author_id_deepwalk_8_final.pkl')\n",
    "deepwalk_author = reduce_mem_usage(deepwalk_author)\n",
    "train = train.merge(deepwalk_author, 'left', 'author_id')\n",
    "\n",
    "time_diff_train = pd.read_hdf('./my_feat/time_diff_train.h5', key='data')\n",
    "print('time_diff_train shape = ', time_diff_train.shape)\n",
    "\n",
    "train = pd.concat([train, time_diff_train], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "94\n",
      "['A1', 'A2', 'B1', 'B2', 'C1', 'C2', 'D1', 'D2', 'E1', 'E2', 'author_id_convert', 'author_id_label_count', 'freq', 'gender', 'invite_hour', 'max_interest_values', 'mean_interest_values', 'min_interest_values', 'most_interest_topic', 'num_atten_topic', 'num_interest_topic', 'score', 'std_interest_values', 'anthor_count', 'question_count', 'num_title_sw', 'num_title_w', 'num_desc_sw', 'num_desc_w', 'num_qtopic', 'question_hour', 'num_topic_attent_intersection', 'num_topic_interest_intersection', 'min_topic_interest_intersection_values', 'max_topic_interest_intersection_values', 'mean_topic_interest_intersection_values', 'std_topic_interest_intersection_values', 'current_answer_count', 'current_invite_count', 'current_invite_q_count', 'topic_vec_probs', 'intersect1d_topic_nums', 'history_topic_vec_distance', 'time_gap', 'history_topic_vec_probs', 'current_invite_user_convert', 'current_invite_user_success_rate', 'uid_label_count', 'qid_label_count', 'uid_day_label_count', 'qid_day_label_count', 'uid_day_hour_label_count', 'qid_day_hour_label_count', 'uid_hour_label_count', 'qid_hour_label_count', 'qid_hour_nunique', 'qid_day_nunique', 'uid_day_nunique', 'uid_hour_nunique', 'uid_seq_feature_3', 'uid_seq_feature_5', 'topic_convert_mean', 'topic_convert_max', 'topic_label_count', 'topic_all_max_count', 'topic_all_mean_count', 'topic_all_min_count', 'topic_all_sum_count', 'k_w_convert_mean', 'k_w_convert_max', 'k_w_convert_min', 'key_words_q_title_label_count', 'kw_max_count', 'kw_mean_count', 'kw_min_count', 'kw_sum_count', 'author_id_question_id_question_id_deepwalk_embedding_8_0', 'author_id_question_id_question_id_deepwalk_embedding_8_1', 'author_id_question_id_question_id_deepwalk_embedding_8_2', 'author_id_question_id_question_id_deepwalk_embedding_8_3', 'author_id_question_id_question_id_deepwalk_embedding_8_4', 'author_id_question_id_question_id_deepwalk_embedding_8_5', 'author_id_question_id_question_id_deepwalk_embedding_8_6', 'author_id_question_id_question_id_deepwalk_embedding_8_7', 'author_id_question_id_author_id_deepwalk_embedding_8_0', 'author_id_question_id_author_id_deepwalk_embedding_8_1', 'author_id_question_id_author_id_deepwalk_embedding_8_2', 'author_id_question_id_author_id_deepwalk_embedding_8_3', 'author_id_question_id_author_id_deepwalk_embedding_8_4', 'author_id_question_id_author_id_deepwalk_embedding_8_5', 'author_id_question_id_author_id_deepwalk_embedding_8_6', 'author_id_question_id_author_id_deepwalk_embedding_8_7', 'merge_uid_time_feature', 'merge_uid_time_past']\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[50]\tvalid_0's auc: 0.858352\n",
      "[100]\tvalid_0's auc: 0.868579\n",
      "[150]\tvalid_0's auc: 0.874837\n",
      "[200]\tvalid_0's auc: 0.87838\n",
      "[250]\tvalid_0's auc: 0.880884\n",
      "[300]\tvalid_0's auc: 0.882695\n",
      "[350]\tvalid_0's auc: 0.884199\n",
      "[400]\tvalid_0's auc: 0.88541\n",
      "[450]\tvalid_0's auc: 0.886485\n",
      "[500]\tvalid_0's auc: 0.887402\n",
      "[550]\tvalid_0's auc: 0.888208\n",
      "[600]\tvalid_0's auc: 0.888934\n",
      "[650]\tvalid_0's auc: 0.889463\n",
      "[700]\tvalid_0's auc: 0.889994\n",
      "[750]\tvalid_0's auc: 0.890408\n",
      "[800]\tvalid_0's auc: 0.890757\n",
      "[850]\tvalid_0's auc: 0.891072\n",
      "[900]\tvalid_0's auc: 0.891313\n",
      "[950]\tvalid_0's auc: 0.891535\n",
      "[1000]\tvalid_0's auc: 0.891706\n",
      "[1050]\tvalid_0's auc: 0.891816\n",
      "[1100]\tvalid_0's auc: 0.891982\n",
      "[1150]\tvalid_0's auc: 0.892125\n",
      "[1200]\tvalid_0's auc: 0.89227\n",
      "[1250]\tvalid_0's auc: 0.892383\n",
      "[1300]\tvalid_0's auc: 0.892533\n",
      "[1350]\tvalid_0's auc: 0.892623\n",
      "[1400]\tvalid_0's auc: 0.892702\n",
      "[1450]\tvalid_0's auc: 0.892802\n",
      "[1500]\tvalid_0's auc: 0.892881\n",
      "[1550]\tvalid_0's auc: 0.892997\n",
      "[1600]\tvalid_0's auc: 0.893083\n",
      "[1650]\tvalid_0's auc: 0.893141\n",
      "[1700]\tvalid_0's auc: 0.893209\n",
      "[1750]\tvalid_0's auc: 0.893286\n",
      "[1800]\tvalid_0's auc: 0.893358\n",
      "[1850]\tvalid_0's auc: 0.893421\n",
      "[1900]\tvalid_0's auc: 0.893508\n",
      "[1950]\tvalid_0's auc: 0.893597\n",
      "[2000]\tvalid_0's auc: 0.893663\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[2000]\tvalid_0's auc: 0.893663\n",
      "time: 23min 44s\n"
     ]
    }
   ],
   "source": [
    "drop_feats = ['question_id', 'author_id', 'author_question_id', 'invite_time', 'label', 'invite_day','old_idx'] + ['mlp_topic_probs_new'] + ['q_i_time_gap_day'] + ['uid_seq_feature']\n",
    "\n",
    "used_feats = [f for f in train.columns if f not in drop_feats]\n",
    "print(len(used_feats))\n",
    "print(used_feats)\n",
    "\n",
    "train_x = train[used_feats].reset_index(drop=True)\n",
    "train_y = train[['label']].reset_index(drop=True)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_X, val_X, train_Y, val_Y = train_test_split(train_x, train_y , test_size=0.2, random_state=42)\n",
    "\n",
    "params = {\n",
    "    'boosting_type': 'gbdt',\n",
    "    'objective': 'binary',\n",
    "    'metric': 'auc',\n",
    "    'num_leaves': 120,\n",
    "    'reg_alpha': 0,\n",
    "    'reg_lambda': 0.,\n",
    "    'max_depth': -1,\n",
    "    'subsample': 0.9,\n",
    "    'colsample_bytree': 0.5,\n",
    "    'subsample_freq': 1,\n",
    "    'learning_rate': 0.035,\n",
    "    'bagging_fraction': 0.8,\n",
    "    'bagging_freq': 5,\n",
    "    'verbose': 1,\n",
    "    'random_state': 42,\n",
    "    'n_jobs': 16,\n",
    "    'device':'gpu'\n",
    "}\n",
    "\n",
    "import lightgbm as lgb\n",
    "\n",
    "lgb_train = lgb.Dataset(train_X, train_Y)\n",
    "lgb_eval = lgb.Dataset(val_X, val_Y, reference=lgb_train)\n",
    "\n",
    "gbm = lgb.train(params,\n",
    "                lgb_train,\n",
    "                num_boost_round=2000,\n",
    "                valid_sets=lgb_eval,\n",
    "                early_stopping_rounds=200,\n",
    "                verbose_eval=50) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1141718, 43)\n",
      "(1141718, 45)\n",
      "(1141718, 47)\n",
      "time: 57.2 s\n"
     ]
    }
   ],
   "source": [
    "test = invite_info_evaluate.merge(question_feat, 'left', 'question_id')\n",
    "test = test.merge(member_question_feat, 'left', 'author_question_id')\n",
    "\n",
    "test = test.merge(current_invite_count, how='left', on=['question_id', 'author_id'])\n",
    "\n",
    "print(test.shape)\n",
    "\n",
    "test_user_current_answer_count =  pd.read_hdf('./my_feat/user_current_answer_count_test_b.h5', key='data')\n",
    "\n",
    "test = test.merge(test_user_current_answer_count, how='left', on=['question_id', 'author_id'])\n",
    "\n",
    "test = test.merge(current_invite_q_count, how='left', on=['question_id', 'author_id'])\n",
    "\n",
    "print(test.shape)\n",
    "\n",
    "topic_cv_probs_test_df = pd.read_hdf('./my_feat/topic_cv_probs_test_df_b.h5',key='data')\n",
    "test = pd.concat([test, topic_cv_probs_test_df],axis=1)\n",
    "\n",
    "# 读取历史topic交集特征\n",
    "history_topic_with_current_topic_test = pd.read_hdf('./my_feat/history_topic_with_current_topic_test_b.h5', key='data')\n",
    "history_topic_with_current_topic_test = history_topic_with_current_topic_test[['question_id','author_id','intersect1d_topic_nums']]\n",
    "\n",
    "test = test.merge(history_topic_with_current_topic_test, how='left', on=['question_id', 'author_id'])\n",
    "\n",
    "print(test.shape)\n",
    "\n",
    "history_topic_vec_distance_test = pd.read_hdf('./my_feat/history_topic_vec_distance_test_b.h5',key='data')\n",
    "test = test.merge(history_topic_vec_distance_test, how='left', on=['question_id', 'author_id'])\n",
    "\n",
    "test_time_gap = pd.read_hdf('./my_feat/test_time_gap_b.h5', key='data')\n",
    "test = pd.concat([test, test_time_gap], axis=1)\n",
    "\n",
    "count_and_unique_feature_test_1124 = pd.read_hdf('./my_feat/count_and_unique_feature_train_final_test.h5', key='data').reset_index(drop=True)\n",
    "test = pd.concat([test, count_and_unique_feature_test_1124], axis=1)\n",
    "\n",
    "\n",
    "history_topic_vec_cv_test = pd.read_hdf('./my_feat/history_topic_vec_cv_test_1118_test_b.h5', key='data')\n",
    "history_topic_vec_cv_test.columns = ['history_topic_vec_probs']\n",
    "test = pd.concat([test, history_topic_vec_cv_test], axis=1)\n",
    "\n",
    "history_invite_situation_test = pd.read_hdf('./my_feat/current_invite_user_situation_test_b.h5', key='data')\n",
    "test = test.merge(history_invite_situation_test, how='left', on=['question_id', 'author_id'])\n",
    "\n",
    "uid_seq_feature_test = pd.read_hdf('./my_feat/uid_seq_feature_test_b.h5', key='data').reset_index(drop=True)\n",
    "\n",
    "test = pd.concat([test, uid_seq_feature_test], axis=1)\n",
    "\n",
    "topic_convert_df_test = pd.read_hdf('./my_feat/topic_convert_test_df_b.h5', key='data')\n",
    "test = pd.concat([test, topic_convert_df_test], axis=1)\n",
    "\n",
    "topic_count_statistics_test = pd.read_hdf('./my_feat/topic_count_statistics_test_b.h5', key='data').reset_index(drop=True)\n",
    "test = pd.concat([test, topic_count_statistics_test], axis=1)\n",
    "\n",
    "\n",
    "kw_count_statistics_test = pd.read_hdf('./my_feat/kw_count_statistics_test_b.h5', key='data')\n",
    "\n",
    "test = pd.concat([test, kw_count_statistics_test], axis=1)\n",
    "\n",
    "q_title_kw_convert_test = pd.read_hdf('./my_feat/q_title_kw_convert_test_b.h5', key='data')\n",
    "\n",
    "test = pd.concat([test, q_title_kw_convert_test],axis=1)\n",
    "\n",
    "test = test.merge(deepwalk_question, 'left', 'question_id')\n",
    "\n",
    "test = test.merge(deepwalk_author, 'left', 'author_id')\n",
    "\n",
    "time_diff_test_b = pd.read_hdf('./my_feat/time_diff_test_b.h5', key='data')\n",
    "\n",
    "test = pd.concat([test, time_diff_test_b], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1141718, 100)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 17.2 ms\n"
     ]
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1141718, 94)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 1.09 s\n"
     ]
    }
   ],
   "source": [
    "test[used_feats].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 2.46 s\n"
     ]
    }
   ],
   "source": [
    "test[used_feats].to_hdf('./my_feat/test_b_feat_sunrui.h5', key='data', index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 34.4 s\n"
     ]
    }
   ],
   "source": [
    "test_x = test[used_feats].reset_index(drop=True)\n",
    "\n",
    "test_pred = gbm.predict(test_x, num_iteration=gbm.best_iteration)\n",
    "\n",
    "result = invite_info_evaluate[['question_id', 'author_id', 'invite_time']]\n",
    "result['result'] = test_pred\n",
    "\n",
    "result.to_csv('./submit/lgb_new.txt', sep='\\t', index=False, header=False)  # 保留三个seq特征  全部，3和5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<lightgbm.basic.Booster at 0x7fabf0ce7748>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 245 ms\n"
     ]
    }
   ],
   "source": [
    "gbm.save_model(filename='./model/lgb_sunrui.model', num_iteration=gbm.best_iteration)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}